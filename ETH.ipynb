{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How results change as $S$ increase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO List\n",
    "+ Plot $S$, $E$ scatter plot?\n",
    "+ Test convergence of optimization by using same $H$ but different starting $\\rho$?\n",
    "+ Larger chain, very slow, need hours to get some result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validity of $E$ vs $S$ plot?\n",
    "+ ETH says local sites are thermalized but not for global, so does global $E\\sim S$ relation make sense\n",
    "+ Hamiltonian is randomly generated, so it is impossible to use single Hamiltonian to get single relationship between $E$ and $S$\n",
    "+ When I calculated the entropy $S(E)$ for Gibbs state, $\\beta$ is small and the $S$ is always near maximal possible entropy $\\lesssim 4$ bit.\n",
    "+ For local sites, what $H$ to use?\n",
    "\n",
    "The Frobenius norm $|\\rho_1-\\rho_2|$ is invariant under unitary transformation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hamiltonian\n",
    "The old $H=-\\sum (Z_iZ_{i+1}+\\Delta X_iX_{i+1})+\\sum g_iX_i$ but it has $\\prod X_i$ conserved. $\\Delta=0.5$ and $g_i$ is sampled uniformly in $[-W, W]$ interval.$\\DeclareMathOperator{\\tr}{tr}$\n",
    "\n",
    "The new $H=-\\sum (X_iX_{i+1}+Y_iY_{i+1}-\\Delta Z_iZ_{i+1})+\\sum (g_iX_i+hZ_i)$ does not have similar conservation. $\\Delta=0.5, h=0.1$, and $g_i$ is also sampled uniformly in a window.\n",
    "\n",
    "## Initial States\n",
    "+ Entropy is evenly divided so every site has entropy $S/n\\in [0, 1]$, with density matrix $\\rho_i=\\mathrm{diag}(p, 1-p)$ such that $\\mathrm{tr}[-\\rho\\log_2\\rho]=S/n$. The initial __global__ density matrix is given by Kronecker product of identical $\\rho_i$ and random unitary transformation $U$: \n",
    "$$\\rho=U\\left[\\bigotimes_{i=1}^n \\rho_i\\right]U^+$$\n",
    "\n",
    "+ Different lines in a figure represents different initial entropy. For each given entropy, there is no __distribution__ of eigenvalues and entropy of $\\rho$. \n",
    "+ The cost function currently being optimized is $\\mathrm{Var}[H]=\\tr[\\rho H^2]-\\tr[\\rho H]^2$. Another energy  fixed version of cost function is $\\mathrm{Var}_E[H]=\\tr[\\rho (H-E)^2]$. \n",
    "+ Distribution of entanglement entropies of the optimized states for some traced is not measured but I can do it later as some optimized $\\rho$ data is right in hand.\n",
    "+ $\\beta$ is not fixed. Currently, $\\beta$ of optimized $\\rho$ is typically very small because $\\beta(E_\\max-E_\\min)\\sim 1$. \n",
    "\n",
    "#### A method to generate initial $\\rho$ with random eigenvalues and fixed  $S_0$\n",
    "+ Step 1: Generate a random $\\rho$ with $L=\\log\\rho$\n",
    "+ Step 2: Tune $\\beta$ to generate $\\rho'=e^{\\beta L}/\\tr[e^{\\beta L}]=\\rho^\\beta/\\tr[\\rho^\\beta]$ satisfying $\\mathrm{tr}[-\\rho'\\log_2\\rho']=S_0$\n",
    "\n",
    "It was used in the past to generate random product density matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ MPO\n",
    "+ pure state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python_3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
